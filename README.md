# Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples
Repository for computing FLS. Currently, mainly supports evaluating image generation but the code can be extended to other data modalities.

## Setup
```bash
git clone https://github.com/marcojira/fls.git
# From venv
cd fls
pip install . 
pip install git+https://github.com/openai/CLIP.git
```

## Quick start
Look at `example.ipynb` to see how to get the FLS of a generative model.
<!-- 
We provide two ways to compute FLS:

```bash
# Usage from the command line
python -m fls --train /path/to/train --test /path/to/test --gen /path/to/generated
```

```python
# Usage from python
from fls.utils import compute_metrics

def gen_samples():
    z = torch.randn(32, 100)
    return G(z).detach()

fls.compute_metrics(train="path/to/train", test="path/to/test", gen=gen_samples)
```

## Advanced usage
Computing FLS requires 3 sets of samples. For each, we recommend using **10000 samples** (less will yield less accurate results and more will run into memory issues due to the computation of the pairwise distances, though switching from full batch gradient descent to SGD will resolve the memory issue). These sets are:
- **Training samples**: Used to train the generative model.
- **Test samples**: From the same distribution as the training samples but were not used to train the generative model
- **Generated samples**: Generated by the generative model

Each of these sets can come from 3 sources:
### Folder of images
Path to a folder containing image samples (either as `.jpg`, `jpeg`, `bmp` or `.png`).

### Pre-computed features in a `.pkl` file
Path to a `.pkl` file which contains a NxD tensor of sample features (where N is the number of samples and D is the dimensionality of the feature space).

### Python function (without arguments) that returns samples
For use from Python (to skip over the time-consuming process of saving samples to disk and then loading them).

```bash
# Usage from the command line combining first two methods
python -m fls --train /path/to/train --test /path/to/test.pkl --gen/path/to/generated
```

```python
# Usage from python combining all three methods.
from fls.utils import compute_metrics

# Example of a function that returns samples from some generated model
def gen_samples():
    # Returned samples must be in a tensor of shape (N, C, H, W)
    z = torch.randn(32, 100)
    # Supposing G is some GAN that takes a latent z
    return G(z).detach() 

fls.compute_metrics(train="path/to/train.", test="path/to/test.pkl", gen=gen_samples)
```

## Process
To compute metrics on samples, the following steps are followed: 
- Samples: e.g. folder of `.png`.
- -> Dataset: torch.utils.Dataset made from the samples.
- -> Features: tensor of size `n x d`. `n` is the number of samples and `d` the dimension of the feature space.
- -> Metrics

The corresponding folders have modules for each of these steps but can be substituted by your own (e.g. if you already have a feature representation of samples, you can pass those directly to a `Metric`).


## File structure
The code is structured as follows:
- `datasets/`: Extensions of PyTorch dataset utilities.
- `features/`: Modules to map samples to different feature spaces
- `metrics/`: Modules for computing various metric values (FLS, FID, AuthPct, CTScore)

## Metrics
By default, only FLS is computed. However, it is possible to also compute the following metrics:
- `"FID"`: Computes the FID score between the training and test samples
- `"AuthPct"`: Percentage of authentic samples (as per https://arxiv.org/abs/2102.08921)
- `"CTScore"`: As defined in https://github.com/casey-meehan/data-copying
- `"FLSoverfit"`: Computes the percentage of overfit Gaussians.

To compute other metrics as well, pass a list of metric names to the `metrics` argument:
```bash
# Usage from the command line combining first two methods
python -m fls --train /path/to/train --test /path/to/test.pkl --gen/path/to/generated --metrics FLS FID AuthPct CTScore FLSoverfit
```

```python
fls.compute_metrics("path/to/train.pkl", "path/to/test.pkl", gen_samples, metrics=["FID", "FLS", "AuthPct", "CTScore", "FLSoverfit"])
```

## Feature caching
By default, computed features are not cached. However, for sets of features that will be used often, it is possible to cache them in a `.pkl` file. For example when you want to monitor FLS during training, the train set and test set are the same and do not to be mapped to the feature space at every epoch; their features can be cached to save time. To do so, pass a path to the `{train,test,gen}_save` argument.

```python
# Calling this for the first time will cache train/test features in their given paths
fls.compute_metrics("path/to/train", "path/to/test", gen_samples, train_save="path/to/train.pkl", test_save="path/to/test.pkl")

# If the features have already been cached, they will be loaded from the given paths
fls.compute_metrics("path/to/train", "path/to/test", gen_samples, train_save="path/to/train.pkl", test_save="path/to/test.pkl" )
```

## Feature space
We provide feature extractors for InceptionV3 (as used in https://github.com/mseitzer/pytorch-fid) and CLIP (https://github.com/openai/CLIP). By default, Inception V3 is used. To change the feature extractor:
```python
from features.CLIPFeatureExtractor import CLIPFeatureExtractor

clip = CLIPFeatureExtractor()
fls.compute_metrics("path/to/train", "path/to/test", gen_samples, feature_extractor=clip)
```

### Adding a new feature space
To create your own FeatureExtractor, create a class that inherits from `features/FeatureExtractor.py`. -->