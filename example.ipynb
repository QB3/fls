{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Simple example of how to get the FLS of a generative model for CIFAR10 (e.g. in this example RandomGAN).\n",
    "\n",
    "We assume the generative model returns images in range $[-1, 1]$ with shape $[B, C, W, H]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class RandomGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RandomGAN, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.randn((128, 3, 32, 32))\n",
    "\n",
    "GAN = RandomGAN()\n",
    "\n",
    "# Create a no-argument function that returns batches of images\n",
    "def generate_imgs():\n",
    "    x = torch.randn((128, 100))\n",
    "    return GAN(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mila/m/marco.jiralerspong/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m test_dataset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR10(\n\u001b[1;32m     14\u001b[0m     root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m\"\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m test_dataset\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCIFAR10_test\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m train_feat \u001b[39m=\u001b[39m feature_extractor\u001b[39m.\u001b[39;49mget_all_features(train_dataset)\n\u001b[1;32m     19\u001b[0m test_feat \u001b[39m=\u001b[39m feature_extractor\u001b[39m.\u001b[39mget_all_features(test_dataset)\n\u001b[1;32m     21\u001b[0m \u001b[39m# For this example, we use RandomGAN\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/gen_model_eval/FLS/fls/fls/features/FeatureExtractor.py:90\u001b[0m, in \u001b[0;36mFeatureExtractor.get_all_features\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns combined features for all chunks\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 90\u001b[0m \u001b[39mfor\u001b[39;00m _, feature \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_features(dataset):\n\u001b[1;32m     91\u001b[0m     features\u001b[39m.\u001b[39mappend(feature)\n\u001b[1;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features)\n",
      "File \u001b[0;32m~/projects/gen_model_eval/FLS/fls/fls/features/FeatureExtractor.py:119\u001b[0m, in \u001b[0;36mFeatureExtractor.get_features\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# Create Subset just for the chunk and get features of that Subset\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     subset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mSubset(dataset, chunk)\n\u001b[0;32m--> 119\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_features(subset)\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_path:\n\u001b[1;32m    121\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_features(features, file_path)\n",
      "File \u001b[0;32m~/projects/gen_model_eval/FLS/fls/fls/features/FeatureExtractor.py:144\u001b[0m, in \u001b[0;36mFeatureExtractor.compute_features\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    141\u001b[0m         features[start_idx:] \u001b[39m=\u001b[39m feature[: size \u001b[39m-\u001b[39m start_idx]\n\u001b[1;32m    142\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     features[start_idx : start_idx \u001b[39m+\u001b[39;49m feature\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]] \u001b[39m=\u001b[39m feature\n\u001b[1;32m    145\u001b[0m     start_idx \u001b[39m=\u001b[39m start_idx \u001b[39m+\u001b[39m feature\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fls.features.DINOv2FeatureExtractor import DINOv2FeatureExtractor # or InceptionFeatureExtractor/CLIPFeatureExtractor\n",
    "from fls.metrics.FLS import FLS\n",
    "\n",
    "# Save path determines where features are cached (useful for train/test sets)\n",
    "feature_extractor = DINOv2FeatureExtractor(save_path=\"data/features\", recompute=True)\n",
    "\n",
    "# FLS needs 3 sets of samples: train, test and gen\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True\n",
    ")\n",
    "train_dataset.name = \"CIFAR10_train\" # Dataset needs a name to cache features\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True\n",
    ")\n",
    "test_dataset.name = \"CIFAR10_test\"\n",
    "\n",
    "train_feat = feature_extractor.get_all_features(train_dataset)\n",
    "test_feat = feature_extractor.get_all_features(test_dataset)\n",
    "\n",
    "# For this example, we use RandomGAN\n",
    "gen_feat = feature_extractor.get_gen_features(generate_imgs, size=10000)\n",
    "\n",
    "# 1.322 is a dataset specific constant\n",
    "cifar_fls = FLS(\"\", 1.322).compute_metric(train_feat, test_feat, gen_feat)\n",
    "print(f\"Random GAN FLS: {cifar_fls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
